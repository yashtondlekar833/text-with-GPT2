# -*- coding: utf-8 -*-
"""GPT-2 Text Generation0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uwMeaVJ_aSWPY__MvnhJsQUHU77QYdnz
"""

# ============================================
# ğŸŒŸ GPT-2 Text Generation Project
# Prodigy Infotech Internship Task (Final Version)
# By: [Pranay Phepade]
# ============================================

# ğŸ§© STEP 1 â€” Install all dependencies (fixed version)
!pip install -U pip setuptools wheel
!pip install tokenizers==0.13.3 --only-binary=:all:
!pip install transformers==4.31.0 datasets torch --quiet

# ğŸ§© STEP 2 â€” Import all libraries
import os
os.environ["WANDB_DISABLED"] = "true"  # disable Weights & Biases pop-up

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset
import torch

# ğŸ§  STEP 3 â€” Load GPT-2 model & tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Fix padding token issue
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# ğŸ“ STEP 4 â€” Create your small dataset
# You can replace these lines with your own text data
text_data = [
    "Artificial intelligence is changing the world of technology.",
    "Machine learning helps computers learn from experience.",
    "Natural language processing enables communication with computers.",
    "Data science combines math and coding to solve real problems.",
    "AI will make automation smarter and more efficient in the future."
]

dataset = Dataset.from_dict({"text": text_data})

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=64)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_datasets  # using all data for training

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âš™ï¸ STEP 5 â€” Set training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    logging_steps=5,
)

# ğŸ‹ï¸â€â™‚ï¸ STEP 6 â€” Train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

print("ğŸš€ Training started... Please wait.")
trainer.train()

# âœ¨ STEP 7 â€” Generate text
prompt = "Artificial intelligence"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_length=80,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

print("\nğŸ§  Generated Text:\n")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# ğŸ’¾ STEP 8 â€” Save model
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

print("\nâœ… Training complete! Model saved in 'fine_tuned_model' folder.")
print("ğŸ‰ You can now use it to generate custom text!")